{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegFormer-B0 Amodal セグメンテーション推論\n",
    "\n",
    "SegFormer-B0モデルを使用した眼領域のAmodal（重なりを許容する）セグメンテーション推論\n",
    "\n",
    "## パイプライン構成\n",
    "1. **Stage 1**: YOLO検出モデルで顔画像から両眼を検出 (Right_eye, Left_eye)\n",
    "2. **Stage 2**: 検出したBBoxからROI（関心領域）を512x512で切り出し\n",
    "3. **Stage 3**: SegFormer-B0でROI画像をセグメンテーション\n",
    "\n",
    "## 出力マスク（3チャンネル、Multi-label）\n",
    "| チャンネル | 名前 | 説明 | 表示色 |\n",
    "|-----------|------|------|--------|\n",
    "| 0 | eyelid | 眼瞼（まぶた）全体 | 赤 |\n",
    "| 1 | iris | 虹彩（完全な楕円） | 緑 |\n",
    "| 2 | pupil | 瞳孔（完全な楕円） | 青 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 1: ライブラリのインポート =====\n",
    "# 必要なライブラリを読み込みます\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO  # YOLOv11検出モデル用\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor  # SegFormerモデル用\n",
    "\n",
    "# PyTorchとGPUの情報を表示\n",
    "print(f\"PyTorch バージョン: {torch.__version__}\")\n",
    "print(f\"CUDA (GPU) 利用可能: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU名: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 2: 設定 =====\n# モデルパスや推論パラメータを設定します\n\n# プロジェクトのルートディレクトリ\nPROJECT_ROOT = Path(\".\").resolve()\n\n# Stage 1: YOLO検出モデル（両眼の位置を検出）\nDETECT_MODEL_PATH = PROJECT_ROOT / \"YOLO11l-detect.pt\"\n\n# Stage 2: SegFormer Amodalモデル（眼領域をセグメンテーション）\nSEGFORMER_MODEL_NAME = \"nvidia/segformer-b0-finetuned-ade-512-512\"  # ベースモデル\nSEGFORMER_MODEL_PATH = PROJECT_ROOT / \"segformer_b0_amodal_best.pth\"\n\n# 推論パラメータ\nIMAGE_SIZE = 512          # ROIのサイズ（512x512ピクセル）\nNUM_CHANNELS = 3          # 出力チャンネル数（eyelid, iris, pupil）\nTHRESHOLD = 0.5           # マスクの2値化閾値（0.5以上を検出領域とする）\nEXPANSION_RATIO = 0.25    # ROI抽出時の拡張率（25%のマージンを追加）\n\n# デバイス設定（GPUがあればGPU、なければCPU）\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# サンプル画像のパス\nSAMPLE_IMAGES = [\n    PROJECT_ROOT / \"sample_image.png\",\n    PROJECT_ROOT / \"sample_image_2.png\",\n]\n\nprint(\"=\" * 50)\nprint(\"設定情報\")\nprint(\"=\" * 50)\nprint(f\"検出モデル: {DETECT_MODEL_PATH}\")\nprint(f\"SegFormerモデル: {SEGFORMER_MODEL_PATH}\")\nprint(f\"デバイス: {device}\")\nprint(f\"ROIサイズ: {IMAGE_SIZE}x{IMAGE_SIZE}\")\nprint(f\"拡張率: {EXPANSION_RATIO} ({int(EXPANSION_RATIO*100)}%)\")\nprint(f\"閾値: {THRESHOLD}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 3: モデルの読み込み =====\n",
    "# Stage1（検出）とStage2（セグメンテーション）のモデルを読み込みます\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"モデルの読み込み\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ----- Stage 1: YOLO検出モデル -----\n",
    "print(\"\\n[Stage 1] YOLO検出モデルを読み込み中...\")\n",
    "detect_model = YOLO(str(DETECT_MODEL_PATH))\n",
    "print(f\"  読み込み完了!\")\n",
    "print(f\"  検出クラス: {detect_model.names}\")\n",
    "\n",
    "# ----- Stage 2: SegFormerモデル -----\n",
    "print(\"\\n[Stage 2] SegFormerモデルを読み込み中...\")\n",
    "\n",
    "def create_segformer_model(num_labels=3):\n",
    "    \"\"\"SegFormer-B0モデルを作成（3チャンネル出力）\"\"\"\n",
    "    model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "        SEGFORMER_MODEL_NAME,\n",
    "        num_labels=num_labels,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# モデルファイルの存在確認\n",
    "if not SEGFORMER_MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"モデルが見つかりません: {SEGFORMER_MODEL_PATH}\")\n",
    "\n",
    "# モデルを作成し、学習済み重みを読み込み\n",
    "seg_model = create_segformer_model(num_labels=NUM_CHANNELS).to(device)\n",
    "checkpoint = torch.load(SEGFORMER_MODEL_PATH, map_location=device, weights_only=False)\n",
    "seg_model.load_state_dict(checkpoint[\"model\"])\n",
    "seg_model.eval()  # 推論モードに設定\n",
    "\n",
    "print(f\"  読み込み完了!\")\n",
    "print(f\"  学習エポック: {checkpoint['epoch']}\")\n",
    "print(f\"  検証Dice: {checkpoint['val_dice']['mean']:.4f}\")\n",
    "print(f\"    - Eyelid: {checkpoint['val_dice']['eyelid']:.4f}\")\n",
    "print(f\"    - Iris: {checkpoint['val_dice']['iris']:.4f}\")\n",
    "print(f\"    - Pupil: {checkpoint['val_dice']['pupil']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 4: 入力画像の読み込みと表示 =====\n# 推論に使用する画像を読み込み、表示します\n\n# 使用する画像（最初のサンプル画像）\nIMAGE_PATH = SAMPLE_IMAGES[0]\n\n# 画像を読み込み\nimg = cv2.imread(str(IMAGE_PATH))\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # BGRからRGBに変換（表示用）\n\nprint(f\"入力画像: {IMAGE_PATH.name}\")\nprint(f\"画像サイズ: {img.shape[1]}x{img.shape[0]} ピクセル\")\n\n# 元画像を表示\nplt.figure(figsize=(10, 10))\nplt.imshow(img_rgb)\nplt.title(f\"Input Image: {IMAGE_PATH.name}\")\nplt.axis('off')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 5: Stage 1 - 眼の検出 =====\n# YOLOモデルで顔画像から両眼の位置を検出します\n\nprint(\"=\" * 50)\nprint(\"Stage 1: 眼の検出\")\nprint(\"=\" * 50)\n\n# 検出を実行\ndetect_results = detect_model.predict(\n    source=img,\n    conf=0.25,   # 信頼度閾値（25%以上を検出）\n    iou=0.45,    # NMS（非最大値抑制）のIoU閾値\n    verbose=False\n)[0]\n\nprint(f\"\\n検出数: {len(detect_results.boxes)} 眼\")\n\n# 検出結果の詳細を表示\nfor i, box in enumerate(detect_results.boxes):\n    cls_id = int(box.cls[0])\n    cls_name = detect_model.names[cls_id]\n    conf = float(box.conf[0])\n    xyxy = box.xyxy[0].cpu().numpy()\n    \n    print(f\"\\n[{i+1}] {cls_name}\")\n    print(f\"    信頼度: {conf:.3f} ({conf*100:.1f}%)\")\n    print(f\"    BBox: x1={xyxy[0]:.0f}, y1={xyxy[1]:.0f}, x2={xyxy[2]:.0f}, y2={xyxy[3]:.0f}\")\n\n# 検出結果を可視化\nresult_img = detect_results.plot()\nresult_img_rgb = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)\n\nplt.figure(figsize=(12, 12))\nplt.imshow(result_img_rgb)\nplt.title(\"Detection Result (Red/Blue: Detected Eyes)\")\nplt.axis('off')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 6: ROI抽出関数の定義 =====\n",
    "# 検出したBBoxから眼領域（ROI）を切り出す関数を定義します\n",
    "\n",
    "def extract_roi(image, bbox, roi_size=512, expansion_ratio=0.25):\n",
    "    \"\"\"\n",
    "    検出したBBoxからROI（関心領域）を抽出\n",
    "    \n",
    "    処理の流れ:\n",
    "    1. BBoxの中心を計算\n",
    "    2. 正方形にするため、長辺に合わせる\n",
    "    3. マージン（expansion_ratio）を追加\n",
    "    4. 画像境界をはみ出す場合は黒でパディング\n",
    "    5. 指定サイズ（roi_size）にリサイズ\n",
    "    \n",
    "    Args:\n",
    "        image: 入力画像（BGR）\n",
    "        bbox: バウンディングボックス [x1, y1, x2, y2]\n",
    "        roi_size: 出力ROIのサイズ（デフォルト512x512）\n",
    "        expansion_ratio: 拡張率（デフォルト25%）\n",
    "    \n",
    "    Returns:\n",
    "        roi: 抽出したROI画像\n",
    "        transform_info: 座標変換用の情報（元画像への逆変換に使用）\n",
    "    \"\"\"\n",
    "    h, w = image.shape[:2]\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # BBoxの中心座標\n",
    "    cx, cy = (x1 + x2) / 2, (y1 + y2) / 2\n",
    "    \n",
    "    # 正方形化 + マージン追加後の辺の長さ\n",
    "    side = max(x2 - x1, y2 - y1) * (1 + 2 * expansion_ratio)\n",
    "    \n",
    "    # 拡張後の座標\n",
    "    nx1, ny1 = int(cx - side / 2), int(cy - side / 2)\n",
    "    nx2, ny2 = int(cx + side / 2), int(cy + side / 2)\n",
    "    \n",
    "    # 画像境界をはみ出す場合のパディング量\n",
    "    pad_top = max(0, -ny1)\n",
    "    pad_bottom = max(0, ny2 - h)\n",
    "    pad_left = max(0, -nx1)\n",
    "    pad_right = max(0, nx2 - w)\n",
    "    \n",
    "    # 画像範囲内のみを切り出し\n",
    "    roi = image[max(0, ny1):min(h, ny2), max(0, nx1):min(w, nx2)].copy()\n",
    "    \n",
    "    # はみ出した部分は黒（0, 0, 0）でパディング\n",
    "    if pad_top or pad_bottom or pad_left or pad_right:\n",
    "        roi = cv2.copyMakeBorder(roi, pad_top, pad_bottom, pad_left, pad_right,\n",
    "                                  cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    \n",
    "    # 座標変換情報を保存（元画像への逆変換に使用）\n",
    "    transform_info = {\n",
    "        'original_bbox': bbox,\n",
    "        'expanded_bbox': [nx1, ny1, nx2, ny2],\n",
    "        'padding': [pad_top, pad_bottom, pad_left, pad_right],\n",
    "        'scale': roi_size / side,\n",
    "        'center': (cx, cy),\n",
    "        'side': side\n",
    "    }\n",
    "    \n",
    "    # 指定サイズにリサイズ\n",
    "    return cv2.resize(roi, (roi_size, roi_size)), transform_info\n",
    "\n",
    "\n",
    "print(\"ROI抽出関数を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 7: Stage 2 - ROIの抽出と表示 =====\n# 検出したBBoxからROIを切り出し、表示します\n\nprint(\"=\" * 50)\nprint(\"Stage 2: ROI抽出\")\nprint(\"=\" * 50)\n\n# 各眼のROIを抽出\nrois = []         # ROI画像を格納\neye_names = []    # 眼の名前（Right_eye / Left_eye）\nbboxes = []       # 元のBBox\ntransform_infos = []  # 座標変換情報\n\nfor box in detect_results.boxes:\n    cls_name = detect_model.names[int(box.cls[0])]\n    bbox = box.xyxy[0].cpu().numpy()\n    \n    # ROIを抽出\n    roi, transform_info = extract_roi(img, bbox, roi_size=IMAGE_SIZE, expansion_ratio=EXPANSION_RATIO)\n    \n    rois.append(roi)\n    eye_names.append(cls_name)\n    bboxes.append(bbox)\n    transform_infos.append(transform_info)\n    \n    print(f\"\\n{cls_name}: ROI抽出完了 ({IMAGE_SIZE}x{IMAGE_SIZE})\")\n\n# 抽出したROIを表示\nfig, axes = plt.subplots(1, len(rois), figsize=(6 * len(rois), 6))\nif len(rois) == 1:\n    axes = [axes]\n\nfor ax, roi, name in zip(axes, rois, eye_names):\n    ax.imshow(cv2.cvtColor(roi, cv2.COLOR_BGR2RGB))\n    ax.set_title(f\"{name} ROI ({IMAGE_SIZE}x{IMAGE_SIZE})\")\n    ax.axis('off')\n\nplt.suptitle(\"Extracted ROI Images\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 8: SegFormer推論関数の定義 =====\n",
    "# SegFormerモデルでROI画像をセグメンテーションする関数を定義します\n",
    "\n",
    "def predict_amodal(model, roi_image, device, image_size=512, threshold=0.5):\n",
    "    \"\"\"\n",
    "    SegFormer Amodalモデルで推論を実行\n",
    "    \n",
    "    Amodal（アモーダル）セグメンテーションとは:\n",
    "    - 各マスク（eyelid, iris, pupil）が独立して予測される\n",
    "    - マスク同士の重なりが許容される（pupil ⊂ iris ⊂ eyelid）\n",
    "    - 瞼で隠れた部分も含めた「完全な」領域を予測\n",
    "    \n",
    "    Args:\n",
    "        model: SegFormerモデル\n",
    "        roi_image: ROI画像（RGBのnumpy配列）\n",
    "        device: 推論デバイス（cuda/cpu）\n",
    "        image_size: モデルの入力サイズ\n",
    "        threshold: 2値化閾値\n",
    "    \n",
    "    Returns:\n",
    "        masks: 各チャンネルの2値マスク {'eyelid', 'iris', 'pupil'}\n",
    "        probs: 各チャンネルの確率マップ\n",
    "    \"\"\"\n",
    "    # RGBに変換（必要な場合）\n",
    "    if roi_image.shape[-1] == 3 and len(roi_image.shape) == 3:\n",
    "        img_rgb = roi_image\n",
    "    else:\n",
    "        img_rgb = cv2.cvtColor(roi_image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # SegFormer用の前処理（正規化など）\n",
    "    processor = SegformerImageProcessor.from_pretrained(\n",
    "        SEGFORMER_MODEL_NAME, do_resize=False, do_rescale=True, do_normalize=True\n",
    "    )\n",
    "    inputs = processor(images=img_rgb, return_tensors=\"pt\")\n",
    "    pixel_values = inputs[\"pixel_values\"].to(device)\n",
    "    \n",
    "    # 推論を実行\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # SegFormerの出力は入力より小さいため、元のサイズに拡大\n",
    "        if logits.shape[-2:] != (image_size, image_size):\n",
    "            logits = F.interpolate(\n",
    "                logits, size=(image_size, image_size),\n",
    "                mode='bilinear', align_corners=False\n",
    "            )\n",
    "        \n",
    "        # Sigmoid関数で確率に変換（Multi-labelなのでSoftmaxではなくSigmoid）\n",
    "        probs = torch.sigmoid(logits).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    # マスクを作成（閾値以上を検出領域とする）\n",
    "    channel_names = ['eyelid', 'iris', 'pupil']\n",
    "    masks = {}\n",
    "    probs_dict = {}\n",
    "    \n",
    "    for c, name in enumerate(channel_names):\n",
    "        probs_dict[name] = probs[c]\n",
    "        masks[name] = ((probs[c] > threshold) * 255).astype(np.uint8)\n",
    "    \n",
    "    return masks, probs_dict\n",
    "\n",
    "\n",
    "print(\"SegFormer推論関数を定義しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 9: Stage 3 - セグメンテーション推論 =====\n",
    "# 各ROIに対してSegFormerでセグメンテーション推論を実行します\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Stage 3: セグメンテーション推論\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 各ROIに対してセグメンテーション推論を実行\n",
    "seg_results = []  # セグメンテーション結果を格納\n",
    "\n",
    "for roi, name in zip(rois, eye_names):\n",
    "    # ROIをRGBに変換\n",
    "    roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 推論を実行\n",
    "    masks, probs = predict_amodal(seg_model, roi_rgb, device, IMAGE_SIZE, THRESHOLD)\n",
    "    \n",
    "    seg_results.append({\n",
    "        'eye_name': name,\n",
    "        'roi_rgb': roi_rgb,\n",
    "        'masks': masks,\n",
    "        'probs': probs\n",
    "    })\n",
    "    \n",
    "    # 結果を表示\n",
    "    total_pixels = IMAGE_SIZE * IMAGE_SIZE\n",
    "    print(f\"\\n{name}: セグメンテーション完了\")\n",
    "    print(f\"  Eyelid: {(masks['eyelid'] > 0).sum():>6} pixels ({100 * (masks['eyelid'] > 0).sum() / total_pixels:.1f}%)\")\n",
    "    print(f\"  Iris:   {(masks['iris'] > 0).sum():>6} pixels ({100 * (masks['iris'] > 0).sum() / total_pixels:.1f}%)\")\n",
    "    print(f\"  Pupil:  {(masks['pupil'] > 0).sum():>6} pixels ({100 * (masks['pupil'] > 0).sum() / total_pixels:.1f}%)\")\n",
    "\n",
    "print(\"\\n全てのROIの推論が完了しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 10: セグメンテーション結果の可視化 =====\n# 各眼のセグメンテーション結果を表示します\n# 表示内容: ROI | Eyelid Mask | Iris Mask | Pupil Mask | Merged\n\ndef create_merged_overlay(img_rgb, eyelid_mask, iris_mask, pupil_mask, alpha=0.5):\n    \"\"\"\n    3つのマスクを重ねたオーバーレイ画像を作成\n    色: Eyelid=赤, Iris=緑, Pupil=青\n    \"\"\"\n    h, w = img_rgb.shape[:2]\n    color_mask = np.zeros((h, w, 3), dtype=np.float32)\n    \n    # 各マスクに色を割り当て\n    color_mask[eyelid_mask > 0, 0] = 255  # 赤: Eyelid\n    color_mask[iris_mask > 0, 1] = 255    # 緑: Iris\n    color_mask[pupil_mask > 0, 2] = 255   # 青: Pupil\n    \n    # 元画像とブレンド\n    overlay = img_rgb.astype(np.float32) * (1 - alpha) + color_mask * alpha\n    overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n    \n    return overlay\n\n\n# 各眼の結果を表示\nfor res in seg_results:\n    roi_rgb = res['roi_rgb']\n    masks = res['masks']\n    eye_name = res['eye_name']\n    \n    # マージ画像を作成\n    merged = create_merged_overlay(roi_rgb, masks['eyelid'], masks['iris'], masks['pupil'])\n    \n    # 5列で表示: ROI | Eyelid | Iris | Pupil | Merged\n    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n    \n    # ROI（元画像）\n    axes[0].imshow(roi_rgb)\n    axes[0].set_title(\"ROI (512x512)\")\n    axes[0].axis(\"off\")\n    \n    # Eyelidマスク（赤系カラーマップ）\n    axes[1].imshow(masks['eyelid'], cmap='Reds', vmin=0, vmax=255)\n    axes[1].set_title(\"Eyelid Mask\")\n    axes[1].axis(\"off\")\n    \n    # Irisマスク（緑系カラーマップ）\n    axes[2].imshow(masks['iris'], cmap='Greens', vmin=0, vmax=255)\n    axes[2].set_title(\"Iris Mask\")\n    axes[2].axis(\"off\")\n    \n    # Pupilマスク（青系カラーマップ）\n    axes[3].imshow(masks['pupil'], cmap='Blues', vmin=0, vmax=255)\n    axes[3].set_title(\"Pupil Mask\")\n    axes[3].axis(\"off\")\n    \n    # マージ画像（3マスクを重畳表示）\n    axes[4].imshow(merged)\n    axes[4].set_title(\"Merged (R=Eyelid, G=Iris, B=Pupil)\")\n    axes[4].axis(\"off\")\n    \n    fig.suptitle(f\"{eye_name} Segmentation Result\", fontsize=14, y=1.02)\n    plt.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell 11: 結果の保存 =====\n",
    "# セグメンテーション結果をファイルに保存します\n",
    "\n",
    "def save_results(image_path, seg_results, output_dir=None):\n",
    "    \"\"\"\n",
    "    セグメンテーション結果をファイルに保存\n",
    "    \n",
    "    保存されるファイル（各眼ごと）:\n",
    "    - {画像名}_{眼名}_roi.png        : 切り出したROI画像\n",
    "    - {画像名}_{眼名}_eyelid_mask.png : Eyelidマスク\n",
    "    - {画像名}_{眼名}_iris_mask.png   : Irisマスク\n",
    "    - {画像名}_{眼名}_pupil_mask.png  : Pupilマスク\n",
    "    - {画像名}_{眼名}_merged.png      : マージ画像\n",
    "    \"\"\"\n",
    "    image_path = Path(image_path)\n",
    "    if output_dir is None:\n",
    "        output_dir = image_path.parent\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    stem = image_path.stem  # 拡張子を除いたファイル名\n",
    "    \n",
    "    for res in seg_results:\n",
    "        eye_name = res['eye_name']\n",
    "        roi_rgb = res['roi_rgb']\n",
    "        masks = res['masks']\n",
    "        merged = create_merged_overlay(roi_rgb, masks['eyelid'], masks['iris'], masks['pupil'])\n",
    "        \n",
    "        # ROI画像を保存（RGBからBGRに変換）\n",
    "        cv2.imwrite(str(output_dir / f\"{stem}_{eye_name}_roi.png\"), \n",
    "                    cv2.cvtColor(roi_rgb, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # 各マスクを保存\n",
    "        cv2.imwrite(str(output_dir / f\"{stem}_{eye_name}_eyelid_mask.png\"), masks['eyelid'])\n",
    "        cv2.imwrite(str(output_dir / f\"{stem}_{eye_name}_iris_mask.png\"), masks['iris'])\n",
    "        cv2.imwrite(str(output_dir / f\"{stem}_{eye_name}_pupil_mask.png\"), masks['pupil'])\n",
    "        \n",
    "        # マージ画像を保存\n",
    "        cv2.imwrite(str(output_dir / f\"{stem}_{eye_name}_merged.png\"), \n",
    "                    cv2.cvtColor(merged, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        print(f\"保存完了: {stem}_{eye_name}_*.png\")\n",
    "\n",
    "\n",
    "# 結果を保存（コメントを外して実行）\n",
    "# save_results(IMAGE_PATH, seg_results)\n",
    "\n",
    "print(\"保存関数を定義しました\")\n",
    "print(\"使い方: save_results(IMAGE_PATH, seg_results)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ===== Cell 12: 別の画像で推論（オプション） =====\n# 他のサンプル画像がある場合、ここで推論を実行できます\n\n# ========================================\n# ここに処理したい画像のパスを指定してください\n# ========================================\nTARGET_IMAGE_PATH = PROJECT_ROOT / \"sample_image_2.png\"\n# TARGET_IMAGE_PATH = Path(r\"C:\\path\\to\\your\\image.png\")  # 任意の画像パス\n# ========================================\n\n\ndef run_full_pipeline(image_path, detect_model, seg_model, device, visualize=True):\n    \"\"\"\n    画像パスを指定して、検出からセグメンテーションまでの全パイプラインを実行\n    \n    Args:\n        image_path: 入力画像のパス\n        detect_model: YOLO検出モデル\n        seg_model: SegFormerセグメンテーションモデル\n        device: 推論デバイス\n        visualize: 結果を表示するかどうか\n    \n    Returns:\n        seg_results: セグメンテーション結果のリスト\n    \"\"\"\n    image_path = Path(image_path)\n    if not image_path.exists():\n        raise FileNotFoundError(f\"画像が見つかりません: {image_path}\")\n    \n    print(f\"\\n{'='*50}\")\n    print(f\"処理中: {image_path.name}\")\n    print(f\"{'='*50}\")\n    \n    # 画像を読み込み\n    img = cv2.imread(str(image_path))\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Stage 1: 検出\n    detect_results = detect_model.predict(source=img, conf=0.25, iou=0.45, verbose=False)[0]\n    print(f\"検出数: {len(detect_results.boxes)} 眼\")\n    \n    if len(detect_results.boxes) == 0:\n        print(\"眼が検出されませんでした\")\n        return []\n    \n    # Stage 2 & 3: ROI抽出とセグメンテーション\n    seg_results = []\n    for box in detect_results.boxes:\n        cls_name = detect_model.names[int(box.cls[0])]\n        bbox = box.xyxy[0].cpu().numpy()\n        \n        roi, _ = extract_roi(img, bbox, IMAGE_SIZE, EXPANSION_RATIO)\n        roi_rgb = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n        masks, probs = predict_amodal(seg_model, roi_rgb, device, IMAGE_SIZE, THRESHOLD)\n        \n        seg_results.append({\n            'eye_name': cls_name,\n            'roi_rgb': roi_rgb,\n            'masks': masks,\n            'probs': probs\n        })\n        print(f\"  {cls_name}: 推論完了\")\n    \n    # 結果を表示\n    if visualize:\n        for res in seg_results:\n            roi_rgb = res['roi_rgb']\n            masks = res['masks']\n            merged = create_merged_overlay(roi_rgb, masks['eyelid'], masks['iris'], masks['pupil'])\n            \n            fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n            axes[0].imshow(roi_rgb); axes[0].set_title(\"ROI\"); axes[0].axis('off')\n            axes[1].imshow(masks['eyelid'], cmap='Reds', vmin=0, vmax=255); axes[1].set_title(\"Eyelid\"); axes[1].axis('off')\n            axes[2].imshow(masks['iris'], cmap='Greens', vmin=0, vmax=255); axes[2].set_title(\"Iris\"); axes[2].axis('off')\n            axes[3].imshow(masks['pupil'], cmap='Blues', vmin=0, vmax=255); axes[3].set_title(\"Pupil\"); axes[3].axis('off')\n            axes[4].imshow(merged); axes[4].set_title(\"Merged\"); axes[4].axis('off')\n            fig.suptitle(f\"{res['eye_name']} - {image_path.name}\", fontsize=14)\n            plt.tight_layout()\n            plt.show()\n    \n    return seg_results\n\n\n# 指定した画像で推論を実行\nif TARGET_IMAGE_PATH.exists():\n    print(f\"指定画像: {TARGET_IMAGE_PATH}\")\n    results_additional = run_full_pipeline(TARGET_IMAGE_PATH, detect_model, seg_model, device)\nelse:\n    print(f\"画像が見つかりません: {TARGET_IMAGE_PATH}\")\n    print(\"TARGET_IMAGE_PATH に有効なパスを指定してください\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まとめ\n",
    "\n",
    "このノートブックでは、SegFormer-B0 Amodalモデルを使用した眼領域のセグメンテーションを行いました。\n",
    "\n",
    "### パイプライン\n",
    "1. **Stage 1**: YOLOで顔画像から両眼を検出 (Right_eye, Left_eye)\n",
    "2. **Stage 2**: 検出したBBoxからROI (512x512) を切り出し\n",
    "3. **Stage 3**: SegFormer-B0でROI画像をセグメンテーション\n",
    "\n",
    "### 出力マスク\n",
    "| チャンネル | 名前 | 説明 | 色 |\n",
    "|-----------|------|------|----|\n",
    "| 0 | eyelid | 眼瞼全体（瞼で隠れた部分含む） | 赤 |\n",
    "| 1 | iris | 虹彩（完全な楕円） | 緑 |\n",
    "| 2 | pupil | 瞳孔（完全な楕円） | 青 |\n",
    "\n",
    "### 主な関数\n",
    "- `extract_roi()`: BBoxからROIを切り出し\n",
    "- `predict_amodal()`: SegFormerでセグメンテーション推論\n",
    "- `create_merged_overlay()`: 3マスクを重畳表示\n",
    "- `save_results()`: 結果をファイルに保存\n",
    "- `run_full_pipeline()`: 検出からセグメンテーションまで一括実行"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}